{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Big Data: Hadoop, Spark, NoSQL and IoT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable high-res images in notebook \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.6 Spark\n",
    "* Use **PySpark** and **Spark functional-style programming** to summarize **word frequencies** in **Romeo and Juliet**\n",
    "* **Hadoop** break tasks into pieces that do **lots of disk I/O across many computers**\n",
    "* **Spark** performs certain big-data tasks **in memory** for **better performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6.1 Spark Overview\n",
    "* In big data, **performance is crucial**\n",
    "* **Hadoop** is geared to **disk-based** batch processing\n",
    "* Many **big-data** applications **demand better performance** \n",
    "    * E.g., **fast streaming** applications requiring **real-time** or **near-real-time processing**  \n",
    "* Spark **in-memory** architecture “**has been used to sort 100 TB of data 3X faster than Hadoop MapReduce on 1/10th of the machines**”[\\[2\\]](https://spark.apache.org/faq.html) \n",
    "* Runs some workloads up to **100 times faster** than Hadoop [\\[3\\]](https://spark.apache.org/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and Components (1 of 2)\n",
    "* Spark uses **resilient distributed datasets (RDDs)** to process distributed data with **functional-style programming** \n",
    "* Hadoop uses **replication for fault tolerance** &mdash; adds **more disk-based overhead**\n",
    "* **RDDs** eliminate disk-based overhead by \n",
    "    * **remaining in memory** &mdash; use disk only if data **can't fit in memory**\n",
    "    * **not replicating data**\n",
    "* **Fault tolerance** &mdash; Spark **remembers steps** used to **create an RDD**\n",
    "    * If a **node fails**, Spark **rebuilds the RDD** [\\[1\\]](https://spark.apache.org/research.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and Components (2 of 2)\n",
    "* **Spark distributes operations** to a cluster’s nodes for **parallel execution**\n",
    "* **Spark streaming** enables you to **process data as it’s received**\n",
    "* **Spark `DataFrame`s** (similar to pandas `DataFrames`), enable you to **manipulate RDDs** as a **collection of named columns**\n",
    "* Can use **Spark `DataFrame`s** with **Spark SQL** to **query distributed data**\n",
    "* **Spark MLlib** (the **Spark Machine Learning Library**) enables you to perform **machine-learning algorithms** on distributed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [**Databricks**](https://databricks.com)\n",
    "* A **Spark-specific vendor**  \n",
    "* Their website is an excellent resource for **learning Spark**\n",
    "* **Paid version** runs on **Amazon AWS** or **Microsoft Azure**\n",
    "* **Free Databricks Community Edition** is a great way to get started with both **Spark** and the **Databricks** environment\n",
    "* [**Databricks free e-books**](https://databricks.com/resources/type/ebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6.2 Docker and the Jupyter Docker Stacks\n",
    "### Docker \n",
    "* Some software packages we use in this chapter require **complicated setup and configuration**\n",
    "* **Preconfigured containers** can help you **get started** with new technologies **quickly and conveniently** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Docker Stacks \n",
    "<!--docker pull jupyter/pyspark-notebook:lab-3.4.7-->\n",
    "* Preconfigured [**Jupyter Docker stacks**](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html) \n",
    "* **`jupyter/pyspark-notebook`** is configured with **Spark and PySpark** \n",
    "* For details on installing and running **Docker** and the **`jupyter/pyspark-notebook`**, see \n",
    "    * [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson16_34)\n",
    "    * [**Python for Programmers** Section 16.6.2](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch16.xhtml#ch16lev2sec25)\n",
    "\n",
    "```\n",
    "docker run -p 8888:8888 -p 4040:4040 -it --user root \\\n",
    "    -v fullPathTo/ch16:/home/jovyan/work \\\n",
    "    jupyter/pyspark-notebook:14fdfbf9cfc1 start.sh jupyter lab\n",
    "```\n",
    "\n",
    "* **Install TextBlob and Tweepy into the container**\n",
    "    * `docker ps`  \n",
    "    **lists running containers**\n",
    "    * `docker exec -it container_name /bin/bash`  \n",
    "    **replace container_name with the name from `docker ps`**\n",
    "    * `conda install textblob tweepy`\n",
    "    \n",
    "<!--\n",
    "docker run -p 8896:8888 -p 4040:4040 -it --user root  -v /Users/pauldeitel/Documents/PythonDataScienceFullThrottle/ch16/:/home/jovyan/work jupyter/pyspark-notebook:14fdfbf9cfc1 start.sh jupyter lab\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6.3 Word Count with Spark\n",
    "* Use Spark **filter, map and reduce** to implement a simple **word count** example that summarizes the words in **Romeo and Juliet**\n",
    "\n",
    "### Loading the NLTK Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a SparkContext \n",
    "* A **`SparkContext`** object gives you access to Spark’s capabilities\n",
    "* Some Spark environments **create a `SparkContext` for you** but not the Jupyter Docker stack\n",
    "* To create a **`SparkContext`**\n",
    "    * Specify the **configuration options** with a **`SparkConf`** object \n",
    "    * **`setMaster`** specifies the **Spark cluster’s URL**\n",
    "    * **`local[*]`** &mdash; Spark is executing on your **`local` computer** \n",
    "    * **`*`** &mdash; Use same number of **threads** as **cores** on your computer\n",
    "        * Simulates **parallelism of Spark clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "configuration = SparkConf().setAppName('RomeoAndJulietCounter')\\\n",
    "                           .setMaster('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(conf=configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Text File and Mapping It to Words\n",
    "* You work with a **`SparkContext`** using **functional-style programming** applied to an **RDD**\n",
    "* **RDD** enables you to **transform the data** stored throughout a **cluster** in **HDFS**\n",
    "* Get a new **`RDD`** representing all words in **Romeo and Juliet**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.utils import strip_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = sc.textFile('RomeoAndJuliet.txt')\\\n",
    "              .flatMap(lambda line: line.lower().split())\\\n",
    "              .map(lambda word: strip_punc(word, all=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "#tokenized = sc.textFile('RomeoAndJuliet.txt')\\\n",
    "#              .map(lambda line: strip_punc(line, all=True).lower())\\\n",
    "#              .flatMap(lambda line: line.split())\n",
    "-->\n",
    "\n",
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the Stop Words\n",
    "* Get a new **`RDD`** with **no stop words** remaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = tokenized.filter(lambda word: word not in stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Each Remaining Word \n",
    "* Now we can **count** the number of **occurrences** of each word\n",
    "* First **`map`** each word to a **tuple** containing the **word** and **`1`**\n",
    "* **`reduceByKey`** with the **`operator`** module’s **`add` function** as an argument **adds** the counts for tuples that contain same **key** (`word`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "word_counts = filtered.map(lambda word: (word, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping Only the Words with Counts Greater Than or Equal to 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_counts = word_counts.filter(lambda item: item[1] >= 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting in Descending Order and Displaying the Results (1 of 2)\n",
    "* At this point, we’ve specified all the steps to **count the words**\n",
    "* When you call an **`RDD`'s `collect` method**, **Spark** \n",
    "    * initiates the **processing steps**\n",
    "    * **returns a list** containing the final results &mdash; **word-count tuples**\n",
    "* Everything **appears to execute on one computer**\n",
    "* Spark **distributes tasks among the cluster’s worker nodes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted_items = sorted(filtered_counts.collect(),\n",
    "                      key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting and Displaying the Results (2 of 2)\n",
    "* We determine the **word with the most letters** so we can **right-align** the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(word) for word, count in sorted_items])\n",
    "for word, count in sorted_items:\n",
    "    print(f'{word:>{max_len}}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terminate current SparkContext so we can create another for next example\n",
    "sc.stop()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Monitoring Interface\n",
    "* https://spark.apache.org/docs/latest/monitoring.html\n",
    "* http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6.4 Spark Word Count on Microsoft Azure\n",
    "* For instructions on how to **run the preceding Spark example using Microsoft Azure HDInsight** see \n",
    "    * [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson16_36) \n",
    "    * [**Python for Programmers, Section 16.6.4**](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch16.xhtml#ch16lev2sec27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.7 Spark Streaming: Counting Twitter Hashtags Using the `pyspark-notebook` Docker Stack\n",
    "* **Stream tweets** and summarize **top-20 hashtags** in dynamically updating **bar chart** \n",
    "* **Spark streaming** will read tweets and **summarize hashtags**\n",
    "* Parts of this sample app communicate with one another via **network sockets**\n",
    "    * low-level **client/server networking** in which a **client** app communicates with a **server** app over a **network** using techniques **similar to file I/O**\n",
    "    * **socket** represents one **endpoint of a connection**\n",
    "* We installed **Tweepy** into the `pyspark-notebook` Docker stack\n",
    "    * For details on installing libraries into **`jupyter/pyspark-notebook`**, see my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson16_34) or [**Python for Programmers** Section 16.6.2](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch16.xhtml#ch16lev2sec25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.7.1 Streaming Tweets to a Socket\n",
    "* The script **`starttweetstream.py`** contains modified **`TweetListener` class** from our Data Mining Twitter presentation\n",
    "* **Streams** the specified number of tweets and **sends them to a socket on the local computer**\n",
    "* Requires **`keys.py`** containing **Twitter credentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the Script in the Docker Container\n",
    "* **File > New > Terminal** \n",
    "* `cd work`\n",
    "* `ipython starttweetstream.py number_of_tweets search_terms`\n",
    ">```\n",
    ">ipython starttweetstream.py 1000 football\n",
    ">```\n",
    "* Script displays `\"Waiting for connection\"` until Spark connects to begin streaming the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class TweetListener (see `starttweetstream.py`)\n",
    "* **Method `on_status`** extracts hashtags, converts to lowercase and creates a space-separated string of hashtags to send to Spark \n",
    "* Uses `connection`’s **`send`** method to send string to whatever is reading from that socket \n",
    "    * **`send`** expects as its argument a **sequence of bytes**\n",
    "    * **`hashtags_string.encode('utf-8')`** converts a string to bytes \n",
    "* Spark streaming automatically reconstructs the strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main Application \n",
    "* Get the **number of tweets to process** \n",
    "* Get **socket object** that we’ll use to wait for a connection from the Spark application\n",
    "* **Bind the socket** to a hostname or IP address and a port number \n",
    "    * Script **listens** for an initial connection  \n",
    "* **Wait until a connection is received** before starting the stream\n",
    "* **`accept`** the connection\n",
    "    * Returns a tuple containing a new **socket object** that the script will use to communicate with the Spark application and the **IP address** of the Spark application’s computer\n",
    "* Authenticate with Twitter and start the stream\n",
    "* Call **`close`** method on the socket objects to release their resources  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.7.2 Summarizing Tweet Hashtags; Introducing Spark SQL\n",
    "* Use **Spark streaming** to read the hashtags sent via a socket by the script `starttweetstream.py` and summarize the results.\n",
    "\n",
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function to Get the SparkSession \n",
    "* Can use **Spark SQL** to query data in **RDDs**\n",
    "* **Spark SQL** uses a **Spark `DataFrame`** to get a **table view** of the underlying **RDDs**\n",
    "* A **`SparkSession`** is used to **create a `DataFrame` from an RDD**\n",
    "* We borrowed the following function from the [**_Spark Streaming Programming Guide_**](https://spark.apache.org/docs/latest/streaming-programming-guide.html#dataframe-and-sql-operations)\n",
    "    * **Defines the correct way to get a `SparkSession` instance** if it already exists or to create one if it does not yet exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSparkSessionInstance(sparkConf):\n",
    "    \"\"\"Spark Streaming Programming Guide's recommended method \n",
    "       for getting an existing SparkSession or creating a new one.\"\"\"\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function to Display a Barchart Based on a Spark DataFrame\n",
    "* Called after Spark processes **each batch of hashtags**\n",
    "* Clears the previous Seaborn barplot, then displays a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_barplot(spark_df, x, y, time, scale=1.0, size=(8, 4.5)):\n",
    "    \"\"\"Displays a Spark DataFrame's contents as a bar plot.\"\"\"\n",
    "    df = spark_df.toPandas()\n",
    "    \n",
    "    # remove prior graph when new one is ready to display\n",
    "    display.clear_output(wait=True) \n",
    "    print(f'TIME: {time}')\n",
    "    \n",
    "    # create and configure a Figure containing a Seaborn barplot \n",
    "    plt.figure(figsize=size)\n",
    "    sns.set(font_scale=scale)\n",
    "    barplot = sns.barplot(data=df, x=x, y=y, \n",
    "                          palette=sns.color_palette('cool', 20))\n",
    "    \n",
    "    # rotate the x-axis labels 90 degrees for readability\n",
    "    for item in barplot.get_xticklabels():\n",
    "        item.set_rotation(90)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function to Summarize the Top-20 Hashtags So Far (1 of 2)\n",
    "* In **Spark streaming**, a **`DStream`** is a **sequence of `RDD`s** \n",
    "* Each **`DStream`** represents **mini-batch** of data to process\n",
    "* Can call a **function** to **perform a task** for **every RDD**\n",
    "* **Function `count_tags`** will \n",
    "    * **summarize hashtag counts** in an **RDD**\n",
    "    * **add** them to the **current totals** (maintained by the **`SparkSession`**)\n",
    "    * display an updated **top-20 barplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(time, rdd):\n",
    "    \"\"\"Count hashtags and display top-20 in descending order.\"\"\"\n",
    "    try:\n",
    "        # get SparkSession\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf()) \n",
    "        \n",
    "        # map hashtag string-count tuples to Rows \n",
    "        rows = rdd.map(\n",
    "            lambda tag: Row(hashtag=tag[0], total=tag[1])) \n",
    "        \n",
    "        # create a DataFrame from the Row objects\n",
    "        hashtags_df = spark.createDataFrame(rows)\n",
    "\n",
    "        # create a temporary table view for use with Spark SQL\n",
    "        hashtags_df.createOrReplaceTempView('hashtags')\n",
    "        \n",
    "        # use Spark SQL to get the top 20 hashtags in descending order\n",
    "        top20_df = spark.sql(\n",
    "            \"\"\"select hashtag, total \n",
    "               from hashtags \n",
    "               order by total desc, hashtag asc \n",
    "               limit 20\"\"\")\n",
    "        display_barplot(top20_df, x='hashtag', y='total', time=time)\n",
    "    except Exception as e:\n",
    "        print(f'Exception: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function to Summarize the Top-20 Hashtags So Far (2 of 2)\n",
    "1. Get the **`SparkSession`** by calling **`getSparkSessionInstance`** with the **`SparkContext`’s configuration information**\n",
    "    * Every **RDD** can access its **`SparkContext`** via the **`context`**\n",
    "2. **`map`** the `RDD`'s data to **`Row`** objects\n",
    "    * `RDD`s in this example contain **tuples of hashtags and counts**\n",
    "    * **`Row` constructor** uses the **keyword argument names** as **column names**\n",
    "3. Create a **Spark `DataFrame`** containing the **`Row` objects** for use with **Spark SQL**\n",
    "4. **Create a table view** of the **`DataFrame`**\n",
    "    * Enables Spark SQL to query a **`DataFrame`** like a table in an **relational database**\n",
    "5. Query the data using **Spark SQL**\n",
    "    * [Details of Spark SQL’s syntax](https://spark.apache.org/sql/)\n",
    "    * **`SparkSession` `sql` method** performs a **query** \n",
    "    * Returns a new Spark **`DataFrame`** containing the results\n",
    "6. Pass the **Spark `DataFrame`** to `display_barplot` utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the `SparkContext` \n",
    "* The rest of the code in this example sets up **Spark streaming** to read text from the **`starttweetstream.py` script** and specifies **how to process the tweets**\n",
    "* Create the **`SparkContext`** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the StreamingContext \n",
    "* For **Spark streaming**, create a **`StreamingContext`** with following arguments\n",
    "    * **`SparkContext`** \n",
    "    * **batch interval** in seconds&mdash;PySpark docs say this **should be at least 10**\n",
    "* For **performance-related issues**, such as **batch intervals**, see the [Performance Tuning section of the **_Spark Streaming Programming Guide_**](https://spark.apache.org/docs/latest/streaming-programming-guide.html#performance-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Checkpoint to Maintain State\n",
    "* Spark streaming **does not maintain state** as you process stream of RDDs\n",
    "* It **_can_ maintain state** via **Spark checkpointing** \n",
    "* Enables **stateful transformations**\n",
    "    * Such as **summarizing collected data** in this example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "ssc.checkpoint(f'checkpoint{time.time()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In a **cloud-based cluster**, you’d specify a **location within HDFS** \n",
    "* In the **Jupyter Docker stack**, Spark creates checkpoint folder in current folder \n",
    "* [More checkpointing details](https://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Stream via a Socket\n",
    "* `StreamingContext` method **`socketTextStream`** **connects to a socket** to receive data stream \n",
    "* Returns a **`DStream`** that receives the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ssc.socketTextStream('localhost', 9876)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Lines of Hashtags\n",
    "* Use **functional style** with a **`DStream`** to specify **processing steps** for **streaming data**\n",
    "* **`flatmap`** space-separated lines of hashtags into **new `DStream`** of **individual hashtags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = stream.flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the Hashtags to Tuples of Hashtag-Count Pairs \n",
    "* **`map`** each hashtag to a **hashtag-count tuple** with an initial count **`1`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = tokenized.map(lambda hashtag: (hashtag, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Totaling the Hashtag Counts So Far\n",
    "* **`updateStateByKey`** receives a **two-argument `lambda`** \n",
    "* **Totals counts** for a given **key** and **adds** them to the **prior total** for that **key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_counts = mapped.updateStateByKey(\n",
    "    lambda counts, prior_total: sum(counts) + (prior_total or 0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the Method to Call for Every RDD\n",
    "* **`foreachRDD`** passes every processed **RDD** to function **`count_tags`**, which **summarizes top-20 hashtags** so far in a **barplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_counts.foreachRDD(count_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Spark Stream\n",
    "* **`StreamingContext`’s `start` method** begins the streaming process\n",
    "\n",
    "<!--\n",
    "### Sample barplot \n",
    "![Sample barplot produced while processing a stream of tweets about football](./ch16images/TwitterHashtags.png \"Sample barplot produced while processing a stream of tweets about football\")\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()  # start the Spark streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 2.x: Structured Streaming\n",
    "* Spark Streaming vs. Structured Streaming:  \n",
    "https://dzone.com/articles/spark-streaming-vs-structured-streaming\n",
    "* Spark: RDD vs DataFrames:  \n",
    "https://blog.knoldus.com/spark-rdd-vs-dataframes/\n",
    "* Structured Streaming Programming Guide:  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Info \n",
    "* See Lesson 16 in [**Python Fundamentals LiveLessons** here on O'Reilly Online Learning](https://learning.oreilly.com/videos/python-fundamentals/9780135917411)\n",
    "* See Chapter 16 in [**Python for Programmers** on O'Reilly Online Learning](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/)\n",
    "* See Chapter 17 in [**Intro Python for Computer Science and Data Science** on O'Reilly Online Learning](https://learning.oreilly.com/library/view/intro-to-python/9780135404799/)\n",
    "* Interested in a print book? Check out:\n",
    "\n",
    "| Python for Programmers<br>(640-page professional book) | Intro to Python for Computer<br>Science and Data Science<br>(880-page college textbook)\n",
    "| :------ | :------\n",
    "| <a href=\"https://amzn.to/2VvdnxE\"><img alt=\"Python for Programmers cover\" src=\"../images/PyFPCover.png\" width=\"150\" border=\"1\"/></a> | <a href=\"https://amzn.to/2LiDCmt\"><img alt=\"Intro to Python for Computer Science and Data Science: Learning to Program with AI, Big Data and the Cloud\" src=\"../images/IntroToPythonCover.png\" width=\"159\" border=\"1\"></a>\n",
    "\n",
    ">Please **do not** purchase both books&mdash;_Python for Programmers_ is a subset of _Intro to Python for Computer Science and Data Science_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
